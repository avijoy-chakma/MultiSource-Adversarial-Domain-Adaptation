{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/avijoychakma/Downloads/DTCN-AR/Packages/Utils/')\n",
    "sys.path.append('/home/avijoychakma/Downloads/DTCN-AR/Packages/Model')\n",
    "\n",
    "from DataPreprocess import onehot_to_label, test_load, load\n",
    "from Visualization import A_plot, B_plot, three_component_pca_visualization, two_component_pca_visualization\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from msda_extractor import AccExtractor\n",
    "from msda_classifier import AccClassifier\n",
    "from matplotlib.lines import Line2D   \n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "    \n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Sources and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_one = 2\n",
    "source_two = 4\n",
    "selected_Target = 1\n",
    "selected_user = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/avijoychakma/Downloads/DTCN-AR/Preprocessing/OPPORTUNITY/ACC-Position-Preprocessed/\"\n",
    "pth_path = \"/home/avijoychakma/Downloads/DTCN-AR/Preprocessing/OPPORTUNITY/ACC-Position-Preprocessed/Trained Model/\"\n",
    "position = [\"BACK\",\"RUA\",\"RLA\",\"LUA\",\"LLA\"]\n",
    "beta1=0.9\n",
    "beta2=0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_label(y_onehot):\n",
    "    a = np.argwhere(y_onehot == 1)\n",
    "    return a[:, -1]\n",
    "\n",
    "def A_plot():\n",
    "    data = np.loadtxt('A_result.csv', delimiter=',')\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(data[:, 0]) + 1), data[:, 0], color='blue', label='train')\n",
    "    plt.plot(range(1, len(data[:, 1]) + 1), data[:, 1], color='red', label='test')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('A Training and Test Accuracy', fontsize=20)\n",
    "    \n",
    "def B_plot():\n",
    "    data = np.loadtxt('B_result.csv', delimiter=',')\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(data[:, 0]) + 1), data[:, 0], color='blue', label='train')\n",
    "    plt.plot(range(1, len(data[:, 1]) + 1), data[:, 1], color='red', label='test')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.title('B Training and Test Accuracy', fontsize=20)\n",
    "    \n",
    "    \n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.5, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.5, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_dataset_train = []\n",
    "RUA_dataset_train = []\n",
    "RLA_dataset_train = []\n",
    "LUA_dataset_train = []\n",
    "LLA_dataset_train = []\n",
    "BACK_gt_train = []\n",
    "RUA_gt_train = []\n",
    "RLA_gt_train = []\n",
    "LUA_gt_train = []\n",
    "LLA_gt_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_dataset_valid = []\n",
    "RUA_dataset_valid = []\n",
    "RLA_dataset_valid = []\n",
    "LUA_dataset_valid = []\n",
    "LLA_dataset_valid = []\n",
    "BACK_gt_valid = []\n",
    "RUA_gt_valid = []\n",
    "RLA_gt_valid = []\n",
    "LUA_gt_valid = []\n",
    "LLA_gt_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size=128\n",
    "step_size=64\n",
    "AXIS = 3\n",
    "FROM = 0\n",
    "TO = FROM+3\n",
    "START = 3\n",
    "END = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = [\"train\",\"valid\"]\n",
    "user = [\"user1\",\"user2\",\"user3\",\"user4\"]\n",
    "position = [\"back\", \"RUA\", \"RLA\", \"LUA\",\"LLA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for position_index in tqdm.tqdm(range(0,5)): #Back, RUA, RLA, LUA, LLA\n",
    "    for split_index in range(0,2):\n",
    "        file_name = user[selected_user-1] + \"_\" + position[position_index]+'_'+item[split_index]\n",
    "        \n",
    "        print(file_name)\n",
    "        df = pd.read_csv(save_path+file_name+'.csv', sep=\",\")   \n",
    "        len_df = df.shape[0]\n",
    "        narray = df.to_numpy()\n",
    "\n",
    "        for i in range(0, len_df, step_size):\n",
    "            window = narray[i:i+win_size, FROM:TO]\n",
    "            \n",
    "            if window.shape[0] != win_size:\n",
    "                continue\n",
    "            else:\n",
    "                reshaped_window = window.reshape(1,win_size,1,AXIS)\n",
    "                gt = np.bincount(narray[i:i+win_size,START:END].astype(int).ravel()).argmax()\n",
    "                \n",
    "                if visualization == 0:\n",
    "                    if position_index == 0:\n",
    "                        if split_index == 0:\n",
    "                            BACK_dataset_train.append(reshaped_window)\n",
    "                            BACK_gt_train.append(gt)\n",
    "                        elif split_index == 1:\n",
    "                            BACK_dataset_valid.append(reshaped_window)\n",
    "                            BACK_gt_valid.append(gt)\n",
    "                    elif position_index == 1:\n",
    "                        if split_index == 0:\n",
    "                            RUA_dataset_train.append(reshaped_window)\n",
    "                            RUA_gt_train.append(gt)\n",
    "                        elif split_index == 1:\n",
    "                            RUA_dataset_valid.append(reshaped_window)\n",
    "                            RUA_gt_valid.append(gt)\n",
    "                    elif position_index == 2:\n",
    "                        if split_index == 0:\n",
    "                            RLA_dataset_train.append(reshaped_window)\n",
    "                            RLA_gt_train.append(gt)\n",
    "                        elif split_index == 1:\n",
    "                            RLA_dataset_valid.append(reshaped_window)\n",
    "                            RLA_gt_valid.append(gt)\n",
    "                    elif position_index == 3:\n",
    "                        if split_index == 0:\n",
    "                            LUA_dataset_train.append(reshaped_window)\n",
    "                            LUA_gt_train.append(gt)\n",
    "                        elif split_index == 1:\n",
    "                            LUA_dataset_valid.append(reshaped_window)\n",
    "                            LUA_gt_valid.append(gt)\n",
    "                    elif position_index == 4:\n",
    "                        if split_index == 0:\n",
    "                            LLA_dataset_train.append(reshaped_window)\n",
    "                            LLA_gt_train.append(gt)\n",
    "                        elif split_index == 1:\n",
    "                            LLA_dataset_valid.append(reshaped_window)\n",
    "                            LLA_gt_valid.append(gt)\n",
    "                else:\n",
    "                    if position_index == 0:\n",
    "                            BACK_dataset_train.append(reshaped_window)\n",
    "                            BACK_gt_train.append(gt)\n",
    "                    elif position_index == 1:\n",
    "                            RUA_dataset_train.append(reshaped_window)\n",
    "                            RUA_gt_train.append(gt)\n",
    "                    elif position_index == 2:\n",
    "                            RLA_dataset_train.append(reshaped_window)\n",
    "                            RLA_gt_train.append(gt)\n",
    "                    elif position_index == 3:\n",
    "                            LUA_dataset_train.append(reshaped_window)\n",
    "                            LUA_gt_train.append(gt)\n",
    "                    elif position_index == 4:\n",
    "                        if split_index == 0:\n",
    "                            LLA_dataset_train.append(reshaped_window)\n",
    "                            LLA_gt_train.append(gt)\n",
    "                        elif split_index == 1:\n",
    "                            LLA_dataset_valid.append(reshaped_window)\n",
    "                            LLA_gt_valid.append(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(BACK_gt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(BACK_gt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source 1 PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = np.concatenate( RLA_dataset_train, axis=0 )\n",
    "pca_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = np.reshape(pca_array, (pca_array.shape[0],(pca_array.shape[1]*pca_array.shape[2]*pca_array.shape[3])))\n",
    "pca_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(pca_array)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PC 1', 'PC 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca3 = PCA(n_components=3)\n",
    "principalComponents3 = pca3.fit_transform(pca_array)\n",
    "principalDf3 = pd.DataFrame(data = principalComponents3, columns = ['PC 1', 'PC 2', 'PC 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUA_gt = np.array(RUA_gt_train)\n",
    "RUA_gt_Df = pd.DataFrame(data = RUA_gt, columns = ['gt'])\n",
    "final_pca_df = pd.concat([principalDf, RUA_gt_Df], axis = 1)\n",
    "final_pca_df3 = pd.concat([principalDf3, RUA_gt_Df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(final_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(RUA_gt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA 2 Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC 1', fontsize = 15)\n",
    "ax.set_ylabel('PC 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0,1,2,3]\n",
    "target_label = ['Sitting','Standing','Lying','Walking']\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = final_pca_df['gt'] == target\n",
    "    ax.scatter(final_pca_df.loc[indicesToKeep, 'PC 1']\n",
    "               , final_pca_df.loc[indicesToKeep, 'PC 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(target_label)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Components PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (8,8))\n",
    "# ax = fig.add_subplot(111, projection='3d') \n",
    "# ax.set_xlabel('PC 1', fontsize = 15)\n",
    "# ax.set_ylabel('PC 2', fontsize = 15)\n",
    "# ax.set_zlabel('PC 3', fontsize = 15)\n",
    "# ax.set_title('3 component PCA', fontsize = 20)\n",
    "# targets = [0,1,2,3]\n",
    "# target_label = ['Sitting','Standing','Lying','Walking']\n",
    "# colors = ['r', 'g', 'b', 'y']\n",
    "\n",
    "# for target, color in zip(targets,colors):\n",
    "#     indicesToKeep = final_pca_df3['gt'] == target\n",
    "#     ax.scatter(final_pca_df3.loc[indicesToKeep, 'PC 1']\n",
    "#                , final_pca_df3.loc[indicesToKeep, 'PC 2']\n",
    "#                , final_pca_df3.loc[indicesToKeep, 'PC 3']\n",
    "#                , c = color\n",
    "#                , s = 50)\n",
    "# ax.legend(target_label)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_len = len(np.unique(S2_GT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_max_len = max(np.unique(S2_GT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source2 PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = np.concatenate( LLA_dataset_train, axis=0 )\n",
    "pca_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = np.reshape(pca_array, (pca_array.shape[0],(pca_array.shape[1]*pca_array.shape[2]*pca_array.shape[3])))\n",
    "pca_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(pca_array)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PC 1', 'PC 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca3 = PCA(n_components=3)\n",
    "principalComponents3 = pca3.fit_transform(pca_array)\n",
    "principalDf3 = pd.DataFrame(data = principalComponents3, columns = ['PC 1', 'PC 2', 'PC 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LUA_gt = np.array(LUA_gt_train)\n",
    "LUA_gt_Df = pd.DataFrame(data = LUA_gt, columns = ['gt'])\n",
    "final_pca_df = pd.concat([principalDf, LUA_gt_Df], axis = 1)\n",
    "final_pca_df3 = pd.concat([principalDf3, LUA_gt_Df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(final_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(LUA_gt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC 1', fontsize = 15)\n",
    "ax.set_ylabel('PC 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0,1,2,3]\n",
    "target_label = ['Sitting','Standing','Lying','Walking']\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = final_pca_df['gt'] == target\n",
    "    ax.scatter(final_pca_df.loc[indicesToKeep, 'PC 1']\n",
    "               , final_pca_df.loc[indicesToKeep, 'PC 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(target_label)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (8,8))\n",
    "# ax = fig.add_subplot(111, projection='3d') \n",
    "# ax.set_xlabel('PC 1', fontsize = 15)\n",
    "# ax.set_ylabel('PC 2', fontsize = 15)\n",
    "# ax.set_zlabel('PC 3', fontsize = 15)\n",
    "# ax.set_title('3 component PCA', fontsize = 20)\n",
    "# targets = [0,1,2,3]\n",
    "# target_label = ['Sitting','Standing','Lying','Walking']\n",
    "# colors = ['r', 'g', 'b', 'y']\n",
    "\n",
    "# for target, color in zip(targets,colors):\n",
    "#     indicesToKeep = final_pca_df3['gt'] == target\n",
    "#     ax.scatter(final_pca_df3.loc[indicesToKeep, 'PC 1']\n",
    "#                , final_pca_df3.loc[indicesToKeep, 'PC 2']\n",
    "#                , final_pca_df3.loc[indicesToKeep, 'PC 3']\n",
    "#                , c = color\n",
    "#                , s = 50)\n",
    "# ax.legend(target_label)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = np.concatenate( BACK_dataset_train, axis=0 )\n",
    "pca_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = np.reshape(pca_array, (pca_array.shape[0],(pca_array.shape[1]*pca_array.shape[2]*pca_array.shape[3])))\n",
    "pca_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(pca_array)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['PC 1', 'PC 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca3 = PCA(n_components=3)\n",
    "principalComponents3 = pca3.fit_transform(pca_array)\n",
    "principalDf3 = pd.DataFrame(data = principalComponents3, columns = ['PC 1', 'PC 2', 'PC 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_gt = np.array(BACK_gt_train)\n",
    "back_gt_Df = pd.DataFrame(data = back_gt, columns = ['gt'])\n",
    "final_pca_df = pd.concat([principalDf, back_gt_Df], axis = 1)\n",
    "final_pca_df3 = pd.concat([principalDf3, back_gt_Df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(final_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(BACK_gt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC 1', fontsize = 15)\n",
    "ax.set_ylabel('PC 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0,1,2,3]\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "target_label = ['Sitting','Standing','Lying','Walking']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = final_pca_df['gt'] == target\n",
    "    ax.scatter(final_pca_df.loc[indicesToKeep, 'PC 1']\n",
    "               , final_pca_df.loc[indicesToKeep, 'PC 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(target_label)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (8,8))\n",
    "# ax = fig.add_subplot(111, projection='3d') \n",
    "# ax.set_xlabel('PC 1', fontsize = 15)\n",
    "# ax.set_ylabel('PC 2', fontsize = 15)\n",
    "# ax.set_zlabel('PC 3', fontsize = 15)\n",
    "# ax.set_title('3 component PCA', fontsize = 20)\n",
    "# targets = [0,1,2,3]\n",
    "# target_label = ['Sitting','Standing','Lying','Walking']\n",
    "# colors = ['r', 'g', 'b', 'y']\n",
    "\n",
    "# for target, color in zip(targets,colors):\n",
    "#     indicesToKeep = final_pca_df3['gt'] == target\n",
    "#     ax.scatter(final_pca_df3.loc[indicesToKeep, 'PC 1']\n",
    "#                , final_pca_df3.loc[indicesToKeep, 'PC 2']\n",
    "#                , final_pca_df3.loc[indicesToKeep, 'PC 3']\n",
    "#                , c = color\n",
    "#                , s = 50)\n",
    "# ax.legend(target_label)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling starts from 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_eye_max = max(np.unique(BACK_gt_train))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_eye_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source1 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source_one == 1:\n",
    "    S1 = np.concatenate( BACK_dataset_train, axis=0 )\n",
    "    S1_GT = np.array(BACK_gt_train  )\n",
    "    S1_Test = np.concatenate( BACK_dataset_valid, axis=0 )\n",
    "    S1_Test_GT = np.array(BACK_gt_valid  )\n",
    "elif source_one == 2:\n",
    "    S1 = np.concatenate( RUA_dataset_train, axis=0 )\n",
    "    S1_GT = np.array(RUA_gt_train  )\n",
    "    S1_Test = np.concatenate(RUA_dataset_valid, axis=0)\n",
    "    S1_Test_GT = np.array(RUA_gt_valid)\n",
    "elif source_one == 3:\n",
    "    S1 = np.concatenate( RLA_dataset_train, axis=0 )\n",
    "    S1_GT = np.array(RLA_gt_train  )\n",
    "    S1_Test = np.concatenate( RLA_dataset_valid, axis=0 )\n",
    "    S1_Test_GT = np.array(RLA_gt_valid  )\n",
    "elif source_one == 4:\n",
    "    S1 = np.concatenate( LUA_dataset_train, axis=0 )\n",
    "    S1_GT = np.array(LUA_gt_train  )\n",
    "    S1_Test = np.concatenate( LUA_dataset_valid, axis=0 )\n",
    "    S1_Test_GT = np.array(LUA_gt_valid  )\n",
    "elif source_one == 5:\n",
    "    S1 = np.concatenate( LLA_dataset_train, axis=0 )\n",
    "    S1_GT = np.array(LLA_gt_train  )\n",
    "    S1_Test = np.concatenate( LLA_dataset_valid, axis=0 )\n",
    "    S1_Test_GT = np.array(LLA_gt_valid  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source2 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source_two == 1:\n",
    "    S2 = np.concatenate( BACK_dataset_train, axis=0 )\n",
    "    S2_GT = np.array(BACK_gt_train  )\n",
    "    S2_Test = np.concatenate( BACK_dataset_valid, axis=0 )\n",
    "    S2_Test_GT = np.array(BACK_gt_valid  )\n",
    "elif source_two == 2:\n",
    "    S2 = np.concatenate( RUA_dataset_train, axis=0 )\n",
    "    S2_GT = np.array(RUA_gt_train  )\n",
    "    S2_Test = np.concatenate(RUA_dataset_valid, axis=0)\n",
    "    S2_Test_GT = np.array(RUA_gt_valid)\n",
    "elif source_two == 3:\n",
    "    S2 = np.concatenate( RLA_dataset_train, axis=0 )\n",
    "    S2_GT = np.array(RLA_gt_train  )\n",
    "    S2_Test = np.concatenate( RLA_dataset_valid, axis=0 )\n",
    "    S2_Test_GT = np.array(RLA_gt_valid  )\n",
    "elif source_two == 4:\n",
    "    S2 = np.concatenate( LUA_dataset_train, axis=0 )\n",
    "    S2_GT = np.array(LUA_gt_train  )\n",
    "    S2_Test = np.concatenate( LUA_dataset_valid, axis=0 )\n",
    "    S2_Test_GT = np.array(LUA_gt_valid  )\n",
    "elif source_two == 5:\n",
    "    S2 = np.concatenate( LLA_dataset_train, axis=0 )\n",
    "    S2_GT = np.array(LLA_gt_train  )\n",
    "    S2_Test = np.concatenate( LLA_dataset_valid, axis=0 )\n",
    "    S2_Test_GT = np.array(LLA_gt_valid  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_Target == 1:\n",
    "    Target = np.concatenate( BACK_dataset_train, axis=0 )\n",
    "    Target_GT = np.array(BACK_gt_train  )\n",
    "elif selected_Target == 2:\n",
    "    Target = np.concatenate(RUA_dataset_train, axis=0)\n",
    "    Target_GT = np.array(RUA_gt_train)\n",
    "elif selected_Target == 3:\n",
    "    Target = np.concatenate( RLA_dataset_train, axis=0 )\n",
    "    Target_GT = np.array(RLA_gt_train  )\n",
    "elif selected_Target == 4:\n",
    "    Target = np.concatenate( LUA_dataset_train, axis=0 )\n",
    "    Target_GT = np.array(LUA_gt_train  )\n",
    "elif selected_Target == 5:\n",
    "    Target = np.concatenate( LLA_dataset_train, axis=0 )\n",
    "    Target_GT = np.array(LLA_gt_train  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gt_number = len(np.unique(S1_GT))\n",
    "output_gt_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1_GT = onehot_to_label(np.eye(numpy_eye_max)[S1_GT])\n",
    "S1_Test_GT = onehot_to_label(np.eye(numpy_eye_max)[S1_Test_GT])\n",
    "S1 =S1.astype(np.float32)\n",
    "S1_GT =S1_GT.astype(np.float32)\n",
    "S1_Test =S1_Test.astype(np.float32)\n",
    "S1_Test_GT =S1_Test_GT.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_GT = onehot_to_label(np.eye(numpy_eye_max)[S2_GT])\n",
    "S2_Test_GT = onehot_to_label(np.eye(numpy_eye_max)[S2_Test_GT])\n",
    "S2 =S2.astype(np.float32)\n",
    "S2_GT =S2_GT.astype(np.float32)\n",
    "S2_Test =S2_Test.astype(np.float32)\n",
    "S2_Test_GT =S2_Test_GT.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target =Target.astype(np.float32)\n",
    "Target_GT = onehot_to_label(np.eye(numpy_eye_max)[Target_GT])\n",
    "Target_GT =Target_GT.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Target_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader(Dataset):\n",
    "    def __init__(self, samples, labels, transform=None):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(train_x, train_y, test_x, test_y, batch_size=64):\n",
    "  \n",
    "    train_x_swap = np.swapaxes(train_x,1,3)\n",
    "    test_x_swap = np.swapaxes(test_x,1,3)\n",
    "\n",
    "    train_set = data_loader(train_x_swap, train_y)\n",
    "    test_set = data_loader(test_x_swap, test_y)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load(test_x, test_y, batch_size=64):\n",
    "\n",
    "    test_x_swap = np.swapaxes(test_x,1,3)\n",
    "    test_set = data_loader(test_x_swap, test_y)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_save_path = pth_path+ \"extractor.pth\"\n",
    "classifier_A_save_path = pth_path + \"classifierA.pth\"\n",
    "classifier_B_save_path = pth_path + \"classifierB.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "N_EPOCH = 250\n",
    "LEARNING_RATE = 0.00001\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "A_result = []\n",
    "B_result = []\n",
    "\n",
    "\n",
    "def train(extractor, classifier_A, classifier_B, optimizer, S1_train, S1_test, S2_train, S2_test):\n",
    "    n_batch = len(S1_train.dataset) // BATCH_SIZE\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for e in range(N_EPOCH):\n",
    "        \n",
    "        extractor.train()\n",
    "        classifier_A.train()\n",
    "        classifier_B.train()\n",
    "        \n",
    "        B_train_itr, B_test_itr = iter(S2_train), iter(S2_test)\n",
    "        \n",
    "        A_correct, B_correct = 0, 0\n",
    "        A_total, B_total = 0, 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        \n",
    "        for index, (A_sample, A_target) in enumerate(S1_train):\n",
    "            \n",
    "            try:\n",
    "                B_sample, B_target = B_train_itr.next()\n",
    "            except StopIteration:\n",
    "                B_train_itr = iter(S2_train)\n",
    "                B_sample, B_target = B_train_itr.next()\n",
    "\n",
    "            \n",
    "            A_sample, A_target = A_sample.to(DEVICE).float(), A_target.to(DEVICE).long()        \n",
    "            A_sample = A_sample.view(-1, AXIS, 1, win_size)\n",
    "            \n",
    "            B_sample, B_target = B_sample.to(DEVICE).float(), B_target.to(DEVICE).long()        \n",
    "            B_sample = B_sample.view(-1, AXIS, 1, win_size)\n",
    "            \n",
    "            \n",
    "            A_feature = extractor(A_sample)\n",
    "            A_output = classifier_A(A_feature)\n",
    "            \n",
    "            B_feature = extractor(B_sample)\n",
    "            B_output = classifier_B(B_feature)\n",
    "            \n",
    "            A_loss = criterion(A_output, A_target)\n",
    "            B_loss = criterion(B_output, B_target)\n",
    "\n",
    "\n",
    "            total_loss = sum([A_loss, B_loss])\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += total_loss.item()\n",
    "            \n",
    "            _, A_predicted = torch.max(A_output.data, 1)\n",
    "            A_total += A_target.size(0)\n",
    "            A_correct += (A_predicted == A_target).sum()\n",
    "            \n",
    "            _, B_predicted = torch.max(B_output.data, 1)\n",
    "            B_total += B_target.size(0)\n",
    "            B_correct += (B_predicted == B_target).sum()\n",
    "\n",
    "            if index % 20 == 0:\n",
    "                tqdm.tqdm.write('Epoch: [{}/{}], Batch: [{}/{}], A_loss:{:.4f}, B_loss:{:.4f}'.format(e + 1,\n",
    "                            N_EPOCH, index + 1, n_batch,A_loss.item(), B_loss.item()))\n",
    "        \n",
    "        A_acc_train = float(A_correct) * 100/ A_total\n",
    "        B_acc_train = float(B_correct) * 100/ B_total\n",
    "\n",
    "        tqdm.tqdm.write('Epoch: [{}/{}], A acc: {:.2f}%, B acc: {:.2f}%'.format(e + 1, N_EPOCH, A_acc_train, B_acc_train))\n",
    "\n",
    "        # Testing\n",
    "        extractor.eval()\n",
    "        classifier_A.eval()\n",
    "        classifier_B.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            A_correct, B_correct = 0, 0\n",
    "            A_total, B_total = 0, 0\n",
    "            correct, total = 0, 0\n",
    "            \n",
    "            for A_sample, A_target in S1_test:\n",
    "                try:\n",
    "                    B_sample, B_target = B_test_itr.next()\n",
    "                except StopIteration:\n",
    "                    B_test_itr = iter(S2_test)\n",
    "                    B_sample, B_target = B_test_itr.next()\n",
    "\n",
    "                \n",
    "                A_sample, A_target = A_sample.to(DEVICE).float(), A_target.to(DEVICE).long()\n",
    "                A_sample = A_sample.view(-1, AXIS, 1, win_size)\n",
    "                \n",
    "                B_sample, B_target = B_sample.to(DEVICE).float(), B_target.to(DEVICE).long()\n",
    "                B_sample = B_sample.view(-1, AXIS, 1, win_size)\n",
    "                \n",
    "                \n",
    "                feature = extractor(A_sample)\n",
    "                A_output = classifier_A(feature)\n",
    "                \n",
    "                feature = extractor(B_sample)\n",
    "                B_output = classifier_B(feature)\n",
    "                \n",
    "                _, A_predicted = torch.max(A_output.data, 1)\n",
    "                A_total += A_target.size(0)\n",
    "                A_correct += (A_predicted == A_target).sum()\n",
    "                \n",
    "                _, B_predicted = torch.max(B_output.data, 1)\n",
    "                B_total += B_target.size(0)\n",
    "                B_correct += (B_predicted == B_target).sum()\n",
    "    \n",
    "        \n",
    "        A_acc_test = float(A_correct) * 100/ A_total\n",
    "        B_acc_test = float(B_correct) * 100/ B_total\n",
    "        tqdm.tqdm.write('Epoch: [{}/{}], A Valid Acc: {:.2f}%, B Valid Acc: {:.2f}%'.format(e + 1, N_EPOCH, A_acc_test, B_acc_test))\n",
    "        \n",
    "        A_result.append([A_acc_train, A_acc_test])\n",
    "        B_result.append([B_acc_train, B_acc_test])\n",
    "        \n",
    "        A_result_np = np.array(A_result, dtype=float)\n",
    "        B_result_np = np.array(B_result, dtype=float)\n",
    "        \n",
    "        np.savetxt('A_result.csv', A_result_np, fmt='%.2f', delimiter=',')\n",
    "        np.savetxt('B_result.csv', B_result_np, fmt='%.2f', delimiter=',')\n",
    "        \n",
    "        torch.save(extractor.state_dict(),feature_save_path)\n",
    "        torch.save(classifier_A.state_dict(),classifier_A_save_path)\n",
    "        torch.save(classifier_B.state_dict(),classifier_B_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(10)\n",
    "    \n",
    "    S1_train, S1_test = load(S1, S1_GT, S1_Test, S1_Test_GT)\n",
    "    S2_train, S2_test = load(S2, S2_GT, S2_Test, S2_Test_GT)\n",
    "\n",
    "    \n",
    "    extractor = AccExtractor().to(DEVICE)\n",
    "    classifier_A = AccClassifier(output_gt_number).to(DEVICE)\n",
    "    classifier_B = AccClassifier(output_gt_number).to(DEVICE)\n",
    "    \n",
    "    param_list = list(extractor.parameters()) + list(classifier_A.parameters()) + list(classifier_B.parameters())\n",
    "    optimizer = optim.Adam(params=param_list, lr=LEARNING_RATE, betas=(beta1, beta2))\n",
    "    train(extractor, classifier_A, classifier_B, optimizer, S1_train, S1_test, S2_train, S2_test)\n",
    "    \n",
    "    A_plot()\n",
    "    B_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor = AccExtractor().to(DEVICE)\n",
    "classifier_A = AccClassifier(output_gt_number).to(DEVICE)\n",
    "classifier_B = AccClassifier(output_gt_number).to(DEVICE)\n",
    "\n",
    "extractor.load_state_dict(torch.load(feature_save_path))\n",
    "classifier_A.load_state_dict(torch.load(classifier_A_save_path))\n",
    "classifier_B.load_state_dict(torch.load(classifier_B_save_path))\n",
    "\n",
    "extractor.eval()\n",
    "classifier_A.eval()\n",
    "classifier_B.eval()\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "target_loader = test_load(Target, Target_GT, batch_size=64)\n",
    "\n",
    "\n",
    "A_correct = 0\n",
    "B_correct = 0\n",
    "A_total = 0\n",
    "B_total = 0\n",
    "\n",
    "accuracy = []\n",
    "for t_sample, t_target in target_loader:\n",
    "\n",
    "    t_sample, t_target = t_sample.to(DEVICE).float(), t_target.to(DEVICE).long()\n",
    "    t_sample = t_sample.view(-1, AXIS, 1, 128)\n",
    "\n",
    "\n",
    "    feature = extractor(t_sample)\n",
    "    A_output = classifier_A(feature)\n",
    "    B_output = classifier_B(feature)\n",
    "\n",
    "    _, A_predicted = torch.max(A_output.data, 1)\n",
    "    A_total += t_target.size(0)\n",
    "    A_correct += (A_predicted == t_target).sum()\n",
    "\n",
    "    _, B_predicted = torch.max(B_output.data, 1)\n",
    "    B_total += t_target.size(0)\n",
    "    B_correct += (B_predicted == t_target).sum()\n",
    "\n",
    "acc_test_A = float(A_correct) * 100 / A_total\n",
    "acc_test_B = float(B_correct) * 100 / B_total\n",
    "tqdm.tqdm.write('A Test Acc: {:.2f}%'.format(acc_test_A))\n",
    "tqdm.tqdm.write('B Test Acc: {:.2f}%'.format(acc_test_B))\n",
    "# accuracy.append([acc_test_A, acc_test_B])\n",
    "# accuracy_np = np.array(accuracy, dtype=float)\n",
    "# np.savetxt('Accuracy.csv', accuracy_np, fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
